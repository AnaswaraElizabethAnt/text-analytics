{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "- Unsupervised model\n",
    "- Supervised model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>UserName</th>\n",
       "      <th>Created_Date</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Lower_Case_Reviews</th>\n",
       "      <th>Sentiment_Manual_BP</th>\n",
       "      <th>Sentiment_Manual</th>\n",
       "      <th>Review_Length</th>\n",
       "      <th>DataSource</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/10/2017</td>\n",
       "      <td>Hh</td>\n",
       "      <td>hh</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2</td>\n",
       "      <td>Google_PlayStore</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/11/2017</td>\n",
       "      <td>No</td>\n",
       "      <td>no</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2</td>\n",
       "      <td>Google_PlayStore</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>asadynwa</td>\n",
       "      <td>8/12/2017</td>\n",
       "      <td>@hotstar_helps during paymnt for premium subsc...</td>\n",
       "      <td>@hotstar_helps during paymnt for premium subsc...</td>\n",
       "      <td>Help</td>\n",
       "      <td>Negative</td>\n",
       "      <td>140</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>jineshroxx</td>\n",
       "      <td>8/11/2017</td>\n",
       "      <td>@hotstartweets I am currently on Jio network a...</td>\n",
       "      <td>@hotstartweets i am currently on jio network a...</td>\n",
       "      <td>Help</td>\n",
       "      <td>Negative</td>\n",
       "      <td>140</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>YaminiSachar</td>\n",
       "      <td>8/5/2017</td>\n",
       "      <td>@hotstartweets the episodes of Sarabhai vs Sar...</td>\n",
       "      <td>@hotstartweets the episodes of sarabhai vs sar...</td>\n",
       "      <td>Help</td>\n",
       "      <td>Negative</td>\n",
       "      <td>140</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID      UserName Created_Date  \\\n",
       "0   1           NaN    8/10/2017   \n",
       "1   2           NaN    8/11/2017   \n",
       "2   3      asadynwa    8/12/2017   \n",
       "3   4    jineshroxx    8/11/2017   \n",
       "4   5  YaminiSachar     8/5/2017   \n",
       "\n",
       "                                             Reviews  \\\n",
       "0                                                 Hh   \n",
       "1                                                 No   \n",
       "2  @hotstar_helps during paymnt for premium subsc...   \n",
       "3  @hotstartweets I am currently on Jio network a...   \n",
       "4  @hotstartweets the episodes of Sarabhai vs Sar...   \n",
       "\n",
       "                                  Lower_Case_Reviews Sentiment_Manual_BP  \\\n",
       "0                                                 hh            Negative   \n",
       "1                                                 no            Negative   \n",
       "2  @hotstar_helps during paymnt for premium subsc...                Help   \n",
       "3  @hotstartweets i am currently on jio network a...                Help   \n",
       "4  @hotstartweets the episodes of sarabhai vs sar...                Help   \n",
       "\n",
       "  Sentiment_Manual  Review_Length        DataSource  Year  Month  Date  \\\n",
       "0         Negative              2  Google_PlayStore  2017      8    10   \n",
       "1         Negative              2  Google_PlayStore  2017      8    11   \n",
       "2         Negative            140           Twitter  2017      8    12   \n",
       "3         Negative            140           Twitter  2017      8    11   \n",
       "4         Negative            140           Twitter  2017      8     5   \n",
       "\n",
       "  Sentiment_Polarity  \n",
       "0            Neutral  \n",
       "1            Neutral  \n",
       "2           Negative  \n",
       "3           Positive  \n",
       "4            Neutral  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotstar_path = 'https://github.com/skathirmani/datasets/raw/master/hotstar.allreviews_Sentiments.csv'\n",
    "hotstar = pd.read_csv(hotstar_path)\n",
    "hotstar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5053, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotstar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     1738\n",
       "Positive    1733\n",
       "Negative    1582\n",
       "Name: Sentiment_Manual, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotstar['Sentiment_Manual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti.polarity_scores('i love india')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6369"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti.polarity_scores('i love india')['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotstar['sentiment_vader'] = hotstar['Reviews'].apply(lambda v: \n",
    "                                                      senti.polarity_scores(v)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hotstar['sentiment_vader'].plot.density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     2149\n",
       "Positive    2137\n",
       "Negative     767\n",
       "Name: sentiment_vader, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_sentiment(v):\n",
    "    if v > 0.25:\n",
    "        return 'Positive'\n",
    "    elif v < -0.25:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "hotstar['sentiment_vader'] = hotstar['sentiment_vader'].apply(assign_sentiment)\n",
    "hotstar['sentiment_vader'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hotstar.columns\n",
    "custom_stop_words = []\n",
    "common_stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words_all = np.hstack([custom_stop_words, common_stop_words])\n",
    "len(stop_words_all)\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5053, 6152)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = hotstar['Reviews']\n",
    "docs = docs.str.lower()\n",
    "docs = docs.str.replace('[^a-z#@ ]', '')\n",
    "docs = docs.str.split(' ')\n",
    "words_rows = docs.tolist()\n",
    "words_all = []\n",
    "words_rows_clean = []\n",
    "docs_clean = []\n",
    "for row in words_rows:\n",
    "    row_words = [stemmer.stem(word) for word in row if word not in stop_words_all]  \n",
    "    words_rows_clean.append(row_words)\n",
    "    docs_clean.append(' '.join(row_words))\n",
    "    words_all.extend(row_words)\n",
    "\n",
    "    \n",
    "model_rf = CountVectorizer()\n",
    "sparse_matrix = model_rf.fit_transform(docs_clean)\n",
    "dtm = pd.DataFrame(sparse_matrix.toarray(),\n",
    "                   columns=model_rf.get_feature_names())\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x = train_test_split(dtm, test_size=0.2, random_state=0)\n",
    "\n",
    "train_y = hotstar.iloc[train_x.index]['Sentiment_Manual']\n",
    "test_y = hotstar.iloc[test_x.index]['Sentiment_Manual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_y.value_counts() / test_y.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_y.value_counts() / train_y.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_rf = model_rf.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.39268051434223"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_y == test_y_rf).sum() / test_y.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.78635014836796"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hotstar = hotstar.iloc[test_y.index]\n",
    "(test_hotstar['Sentiment_Manual'] == \n",
    " test_hotstar['sentiment_vader']).sum() / test_hotstar.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.34817012858556"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(train_x, train_y)\n",
    "test_y_pred = model.predict(test_x)\n",
    "(test_y_pred == test_y).sum() / test_y.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.710187932739856"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(train_x, train_y)\n",
    "test_y_pred = model.predict(test_x)\n",
    "(test_y_pred == test_y).sum() / test_y.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter API (REST API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.0'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install tweepy\n",
    "import tweepy\n",
    "tweepy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ''\n",
    "api_secret = ''\n",
    "access_token = ''\n",
    "access_secret = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.OAuthHandler(api_key, api_secret)\n",
    "api.set_access_token(access_token, access_secret)\n",
    "\n",
    "api_auth = tweepy.API(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipkart_tweets = api_auth.search('#flipkart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flipkart_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweepy.models.SearchResults"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(flipkart_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@bidbud68 And #Multichoice #Flipkart #Naspers'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipkart_tweets[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 11, 7, 6, 47, 58)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipkart_tweets[0].created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@bidbud68 And #Multichoice #Flipkart #Naspers 2018-11-07 06:47:58 Zenzo L\n",
      "@flipkartsupport @Flipkart @_sachinbansal @binnybansal #please #UPDATE #waiting for #Reply #more than #24hours pass… https://t.co/84WkeYRo6x 2018-11-07 06:40:45 Siddiqui Farhan 🇮🇳\n",
      "#Happy_Diwali\n",
      "#ankit #ankitraj\n",
      "#ankitrajinararia\n",
      "#growerfly\n",
      "#ajitjha  #theankitraj #dtscindia #HappyDiwali #araria… https://t.co/CFkeWSs6HH 2018-11-07 06:24:18 Ankit Raj\n",
      "Participate in Guess What? on Flipkart and exciting rewards. #flipkart #guesswhat #gamezone\n",
      "https://t.co/A87YpPnxKc 2018-11-07 06:23:39 Azeem pasha\n",
      "ಆನ್‌ಲೈನ್‌ನಲ್ಲಿ ಮಾರಾಟವಾಗುವ ಉತ್ಪನ್ನಗಳೆಲ್ಲ ಅಸಲಿಯಲ್ಲ..! https://t.co/664GWGrMpA #online #amazon #flipkart #news… https://t.co/pzDaxt13SL 2018-11-07 06:09:31 Oneindia Kannada\n",
      "Participate in Flipkart's LIVE quiz game and win gift vouchers, TVs, mobiles and more! 8:00 PM daily. #flipkart… https://t.co/fqEqixIGOf 2018-11-07 06:08:04 Rajesh Bishnoi\n",
      "फ्लिपकार्ट और अमेजन की त्‍योहारों में हुई बंपर सेल  https://t.co/9Er6aJZOZd #flipkart #amazonindia #फ्लिपकार्ट #अमेजनइंडिया 2018-11-07 05:53:08 Goodreturns Hindi\n",
      "@flipkartsupport @binnybansal @_sachinbansal @marcericlore Is anyone in #Flipkart working on this? Or have you aban… https://t.co/2vu8pCjo3z 2018-11-07 05:45:34 Feby Daniel\n",
      "Buy these handcrafted hammocks online at https://t.co/DoEfpPvj1B | #shopping #sale #deals #ecommerce… https://t.co/apf0zFwGvF 2018-11-07 05:35:14 Hangit Hammock Swing\n",
      "RT @BeExaltic: UPTO 30% Discounts this Diwali Season on All Exaltic Nutrition Products #beExaltic #workout #fitness #fitindia #workoutmotiv… 2018-11-07 05:32:04 Prashant Kashyap\n",
      "ఆఫర్ల పేరుతో భారీ మోసాలు అన్నీ ఫేక్ వస్తువులే ! https://t.co/6iDniTdx1u #online #paytm #flipkart #amazon #snapdeal… https://t.co/7H5qL1I0QM 2018-11-07 05:13:54 Oneindia Telugu\n",
      "ఆఫర్ల పేరుతో భారీ మోసాలు అన్నీ ఫేక్ వస్తువులే ! https://t.co/XFCbrf4pPC #online #paytm #flipkart #amazon #snapdeal… https://t.co/XiTFk5H0Ph 2018-11-07 05:13:53 Gizbot Telugu\n",
      "Register New Account You Will Get Rs.1000 #Fynd Cash\n",
      "\n",
      "Click on My Account Icon Then Click Fynd Cash\n",
      "\n",
      "Then Go Refer… https://t.co/eJFdGhMixb 2018-11-07 05:10:53 Deal Finder\n"
     ]
    }
   ],
   "source": [
    "for tweet in flipkart_tweets:\n",
    "    print(tweet.text, tweet.created_at, tweet.user.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
