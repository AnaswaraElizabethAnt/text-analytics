{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "- Unsupervised model\n",
    "- Supervised model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>UserName</th>\n",
       "      <th>Created_Date</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Lower_Case_Reviews</th>\n",
       "      <th>Sentiment_Manual_BP</th>\n",
       "      <th>Sentiment_Manual</th>\n",
       "      <th>Review_Length</th>\n",
       "      <th>DataSource</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/10/2017</td>\n",
       "      <td>Hh</td>\n",
       "      <td>hh</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2</td>\n",
       "      <td>Google_PlayStore</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/11/2017</td>\n",
       "      <td>No</td>\n",
       "      <td>no</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2</td>\n",
       "      <td>Google_PlayStore</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>asadynwa</td>\n",
       "      <td>8/12/2017</td>\n",
       "      <td>@hotstar_helps during paymnt for premium subsc...</td>\n",
       "      <td>@hotstar_helps during paymnt for premium subsc...</td>\n",
       "      <td>Help</td>\n",
       "      <td>Negative</td>\n",
       "      <td>140</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>jineshroxx</td>\n",
       "      <td>8/11/2017</td>\n",
       "      <td>@hotstartweets I am currently on Jio network a...</td>\n",
       "      <td>@hotstartweets i am currently on jio network a...</td>\n",
       "      <td>Help</td>\n",
       "      <td>Negative</td>\n",
       "      <td>140</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>YaminiSachar</td>\n",
       "      <td>8/5/2017</td>\n",
       "      <td>@hotstartweets the episodes of Sarabhai vs Sar...</td>\n",
       "      <td>@hotstartweets the episodes of sarabhai vs sar...</td>\n",
       "      <td>Help</td>\n",
       "      <td>Negative</td>\n",
       "      <td>140</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID      UserName Created_Date  \\\n",
       "0   1           NaN    8/10/2017   \n",
       "1   2           NaN    8/11/2017   \n",
       "2   3      asadynwa    8/12/2017   \n",
       "3   4    jineshroxx    8/11/2017   \n",
       "4   5  YaminiSachar     8/5/2017   \n",
       "\n",
       "                                             Reviews  \\\n",
       "0                                                 Hh   \n",
       "1                                                 No   \n",
       "2  @hotstar_helps during paymnt for premium subsc...   \n",
       "3  @hotstartweets I am currently on Jio network a...   \n",
       "4  @hotstartweets the episodes of Sarabhai vs Sar...   \n",
       "\n",
       "                                  Lower_Case_Reviews Sentiment_Manual_BP  \\\n",
       "0                                                 hh            Negative   \n",
       "1                                                 no            Negative   \n",
       "2  @hotstar_helps during paymnt for premium subsc...                Help   \n",
       "3  @hotstartweets i am currently on jio network a...                Help   \n",
       "4  @hotstartweets the episodes of sarabhai vs sar...                Help   \n",
       "\n",
       "  Sentiment_Manual  Review_Length        DataSource  Year  Month  Date  \\\n",
       "0         Negative              2  Google_PlayStore  2017      8    10   \n",
       "1         Negative              2  Google_PlayStore  2017      8    11   \n",
       "2         Negative            140           Twitter  2017      8    12   \n",
       "3         Negative            140           Twitter  2017      8    11   \n",
       "4         Negative            140           Twitter  2017      8     5   \n",
       "\n",
       "  Sentiment_Polarity  \n",
       "0            Neutral  \n",
       "1            Neutral  \n",
       "2           Negative  \n",
       "3           Positive  \n",
       "4            Neutral  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotstar_path = 'https://github.com/skathirmani/datasets/raw/master/hotstar.allreviews_Sentiments.csv'\n",
    "hotstar = pd.read_csv(hotstar_path)\n",
    "hotstar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5053, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotstar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     1738\n",
       "Positive    1733\n",
       "Negative    1582\n",
       "Name: Sentiment_Manual, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotstar['Sentiment_Manual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti.polarity_scores('i love india')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6369"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti.polarity_scores('i love india')['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotstar['sentiment_vader'] = hotstar['Reviews'].apply(lambda v: \n",
    "                                                      senti.polarity_scores(v)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hotstar['sentiment_vader'].plot.density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     2149\n",
       "Positive    2137\n",
       "Negative     767\n",
       "Name: sentiment_vader, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_sentiment(v):\n",
    "    if v > 0.25:\n",
    "        return 'Positive'\n",
    "    elif v < -0.25:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "hotstar['sentiment_vader'] = hotstar['sentiment_vader'].apply(assign_sentiment)\n",
    "hotstar['sentiment_vader'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hotstar.columns\n",
    "custom_stop_words = []\n",
    "common_stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words_all = np.hstack([custom_stop_words, common_stop_words])\n",
    "len(stop_words_all)\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5053, 6152)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = hotstar['Reviews']\n",
    "docs = docs.str.lower()\n",
    "docs = docs.str.replace('[^a-z#@ ]', '')\n",
    "docs = docs.str.split(' ')\n",
    "words_rows = docs.tolist()\n",
    "words_all = []\n",
    "words_rows_clean = []\n",
    "docs_clean = []\n",
    "for row in words_rows:\n",
    "    row_words = [stemmer.stem(word) for word in row if word not in stop_words_all]  \n",
    "    words_rows_clean.append(row_words)\n",
    "    docs_clean.append(' '.join(row_words))\n",
    "    words_all.extend(row_words)\n",
    "\n",
    "    \n",
    "model_dtm = CountVectorizer()\n",
    "sparse_matrix = model_dtm.fit_transform(docs_clean)\n",
    "dtm = pd.DataFrame(sparse_matrix.toarray(),\n",
    "                   columns=model_dtm.get_feature_names())\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x = train_test_split(dtm, test_size=0.2, random_state=0)\n",
    "\n",
    "train_y = hotstar.iloc[train_x.index]['Sentiment_Manual']\n",
    "test_y = hotstar.iloc[test_x.index]['Sentiment_Manual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_y.value_counts() / test_y.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_y.value_counts() / train_y.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_rf = model_rf.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.39268051434223"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_y == test_y_rf).sum() / test_y.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.78635014836796"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hotstar = hotstar.iloc[test_y.index]\n",
    "(test_hotstar['Sentiment_Manual'] == \n",
    " test_hotstar['sentiment_vader']).sum() / test_hotstar.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.34817012858556"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(train_x, train_y)\n",
    "test_y_pred = model.predict(test_x)\n",
    "(test_y_pred == test_y).sum() / test_y.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.710187932739856"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(train_x, train_y)\n",
    "test_y_pred = model.predict(test_x)\n",
    "(test_y_pred == test_y).sum() / test_y.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter API (REST API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.0'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install tweepy\n",
    "import tweepy\n",
    "tweepy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ''\n",
    "api_secret = ''\n",
    "access_token = ''\n",
    "access_secret = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.OAuthHandler(api_key, api_secret)\n",
    "api.set_access_token(access_token, access_secret)\n",
    "\n",
    "api_auth = tweepy.API(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipkart_tweets = api_auth.search('#flipkart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flipkart_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweepy.models.SearchResults"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(flipkart_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@bidbud68 And #Multichoice #Flipkart #Naspers'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipkart_tweets[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 11, 7, 6, 47, 58)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipkart_tweets[0].created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/skathirmani/text-analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipkart_tweets = api_auth.search('#flipkart', count=100)\n",
    "flipkart_df = pd.DataFrame(columns=['text', 'created_at', 'name'])\n",
    "for tweet in flipkart_tweets:\n",
    "    curr_row = {'text': tweet.text,\n",
    "                'created_at': tweet.created_at,\n",
    "                'name': tweet.user.name}\n",
    "    flipkart_df = flipkart_df.append(curr_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @nutramantra: Happy Diwali. Stay Happy &amp;amp...</td>\n",
       "      <td>2018-11-07 07:04:41</td>\n",
       "      <td>jig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Flipkart I was suppose to get my order by 7th...</td>\n",
       "      <td>2018-11-07 07:01:25</td>\n",
       "      <td>Utpal Yadav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@narendramodi @ITBP_official कुछ secular लोग #...</td>\n",
       "      <td>2018-11-07 06:54:24</td>\n",
       "      <td>Sumesh Raj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@flipkartsupport Not #expected this #type of #...</td>\n",
       "      <td>2018-11-07 06:53:32</td>\n",
       "      <td>Siddiqui Farhan 🇮🇳</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@bidbud68 And #Multichoice #Flipkart #Naspers</td>\n",
       "      <td>2018-11-07 06:47:58</td>\n",
       "      <td>Zenzo L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          created_at  \\\n",
       "0  RT @nutramantra: Happy Diwali. Stay Happy &amp... 2018-11-07 07:04:41   \n",
       "1  @Flipkart I was suppose to get my order by 7th... 2018-11-07 07:01:25   \n",
       "2  @narendramodi @ITBP_official कुछ secular लोग #... 2018-11-07 06:54:24   \n",
       "3  @flipkartsupport Not #expected this #type of #... 2018-11-07 06:53:32   \n",
       "4      @bidbud68 And #Multichoice #Flipkart #Naspers 2018-11-07 06:47:58   \n",
       "\n",
       "                 name  \n",
       "0                 jig  \n",
       "1         Utpal Yadav  \n",
       "2          Sumesh Raj  \n",
       "3  Siddiqui Farhan 🇮🇳  \n",
       "4             Zenzo L  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipkart_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotstar_tweets = api_auth.search('#hotstar', count=500)\n",
    "hotstar_df = pd.DataFrame(columns=['text', 'created_at', 'name'])\n",
    "for tweet in hotstar_tweets:\n",
    "    curr_row = {'text': tweet.text,\n",
    "                'created_at': tweet.created_at,\n",
    "                'name': tweet.user.name}\n",
    "    hotstar_df = hotstar_df.append(curr_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotstar_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotstar.to_csv('hotstar_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting sentiment for new tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = hotstar_df['text'].str.lower()\n",
    "docs_new = docs_new.str.replace('[^a-z#@ ]', '')\n",
    "docs_new = docs_new.str.split(' ')\n",
    "words_rows = docs_new.tolist()\n",
    "\n",
    "docs_clean_new = []\n",
    "for row in words_rows:\n",
    "    row_words = [stemmer.stem(word) for word in row if word not in stop_words_all]  \n",
    "    docs_clean_new.append(' '.join(row_words))\n",
    "\n",
    "model_new = CountVectorizer(vocabulary=model_dtm.vocabulary_)\n",
    "dtm_new = pd.DataFrame(model_new.fit_transform(docs_clean_new).toarray(),\n",
    "                      columns=model_dtm.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_rf.predict(dtm_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(model_dtm.get_feature_names())\n",
    "#np.sort(list(model_dtm.vocabulary_.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency (TF - IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.77744807121661"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "model_train_tfidf = TfidfVectorizer()\n",
    "dtm_tfidf = pd.DataFrame(model_train_tfidf.fit_transform(docs_clean).toarray(),\n",
    "                         columns=model_train_tfidf.get_feature_names())\n",
    "\n",
    "train_x, test_x = train_test_split(dtm_tfidf, test_size=0.2, random_state=0)\n",
    "\n",
    "train_y = hotstar.iloc[train_x.index]['Sentiment_Manual']\n",
    "test_y = hotstar.iloc[test_x.index]['Sentiment_Manual']\n",
    "\n",
    "model_rf2 = RandomForestClassifier()\n",
    "model_rf2.fit(train_x, train_y)\n",
    "\n",
    "pred = model_rf2.predict(test_x)\n",
    "(pred == test_y).sum() / test_y.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.4807121661721"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
